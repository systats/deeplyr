---
title: "Trainer"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Packages

```{r}
pacman::p_load(tidyverse, purrr, furrr, keras, xgboost)
devtools::document()
devtools::load_all()
options(scipen = 999)
# devtools::install()
```


# Main Function

```{r}
split_sample <- function(x, y, id = NULL, val = F, bet = F, oos = F, balance = NULL){
  sp <- splitter$new()
  sp$set(x, y, id)
  sp$split(val, bet, oos, balance)
  return(sp)
}
```



# Test Data

```{r}
df <- tibble(
  id = sample(1:100, 1000, replace = T),
  split_id = sample(1:3, size = 1000, prob = c(.8, .1, .1), replace = T),
  x1 = rnorm(1000),
  x2 = rnorm(1000),
  z = 2*x1 + 3*x2,
  y = ifelse(z > mean(z), 1, 0), #2*x1 + 2*x2 + rnorm(1000)
  y_multi0 = case_when(z > (mean(z)+.2*sd(z)) ~ 0, z < (mean(z)-.2*sd(z)) ~ 2, T ~ 1),
  y_multi1 = y_multi0 + 1
) %>% 
  select(split_id, contains("y"), id, contains("x")) %>%
  glimpse
```

```{r}
### Data
sp <- split_sample(x = df[,c("x1", "x2")], y = df$y, )
sp$splits %>% iwalk(glimpse)
```


# Trainer

## XGBoost

```{r}
devtools::document()
fit1 <- fit_learner(list(), sp$splits, "xgboost:binary")
fit1$eval$preds
```


## Keras

```{r}
devtools::document()
# custom_keras <- function(input_dim = 20, output_dim = 1, output_fun = "sigmoid"){
#   keras::keras_model_sequential() %>%
#     keras::layer_dense(units = 10, activation = "relu", input_shape = input_dim) %>%
#     keras::layer_dense(units = output_dim, activation = output_fun)
# }
#pkeras <- list(input_dim = 2, output_dim = 1, output_fun = "sigmoid", model = custom_keras)
fit1 <- fit_learner(list(), sp$splits, "keras:binary")
fit1$eval$preds
```


## RANGER

```{r}
devtools::document()
fit1 <- fit_learner(list(min_node_size = 5), sp$splits, "ranger:binary", dev = F)
fit1$test()
fit1$eval$preds
```


## CATBOOST

* catboost-darwin-0.17.1
* catboost-R-Darwin-0.17.1.tgz
* devtools::install_url('https://github.com/catboost/catboost/releases/download/v0.17.1/catboost-R-Darwin-0.17.1.tgz', args = c("--no-multiarch"))


```{r}
devtools::document()
fit1 <- fit_learner(NULL, sp$splits, "catboost:binary")
fit1$eval$preds
```



## RPART

```{r}
devtools::document()
#rpart::rpart()
#rpart::rpart.control()
fit1 <- fit_learner(NULL, sp$splits, "rpart:binary")
fit1$eval$preds
```


## randomForest

```{r}
devtools::document()
fit1 <- fit_learner(NULL, sp$splits, "rf:binary")
fit1$eval$preds
```





# All together

```{r}
all <- c("keras:binary", "xgboost:binary", "catboost:binary", "ranger:binary", "rpart:binary", "rf:binary") %>% 
  imap(~{
    fit1 <- fit_learner(NULL, sp$splits, .x)
    pack <- .x %>% str_extract("\\w+")
    fit1$eval$preds  %>% select(id, target, everything()) %>% rename_at(-1:-2, ~paste0(.x, "_", pack))
  }) %>%
  reduce(left_join)

all %>% 
  select(contains("pred")) %>% 
  imap(~{
    mean(.x == all$target)
  })

sp_new <- split_sample(x = all %>% select(contains("prob"), -contains("keras")), y = all$target)
fit1 <- fit_learner(NULL, sp_new$splits, "xgboost:binary")
mean(fit1$eval$preds$pred == fit1$eval$preds$target)
```




# Other


## LightGBM

* https://github.com/microsoft/LightGBM/tree/master/R-package
**Not Working Right Now**

### Either use Docker

```{bash}
mkdir lightgbm-docker
cd lightgbm-docker
wget https://raw.githubusercontent.com/Microsoft/LightGBM/master/docker/dockerfile-r
docker build -t lightgbm-r -f dockerfile-r .
```

### Or setup up your system

```{bash}
brew install cmake
brew install libomp
brew install cmake
brew install gcc
```

```{bash}
git clone --recursive https://github.com/microsoft/LightGBM
cd LightGBM
Rscript build_r.R
```


### Then Install R Package

```{r}
# devtools::install_github("Microsoft/LightGBM", subdir = "R-package")
# devtools::install_github("Laurae2/lgbdl")
lgbdl::lgb.dl(commit = "master",
       compiler = "vs",
       repo = "https://github.com/microsoft/LightGBM")
```


```{r}
library(lightgbm)
data(agaricus.train, package='lightgbm')
dtrain <- lgb.Dataset(as.matrix(mtcars[,1:2]), label=mtcars$vs)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=1, early_stopping_rounds=10)
```






## GBM3

```{r}
# devtools::install_github("gbm-developers/gbm3")
# library(gbm3)
# available_distributions()
# bern_dist <- gbm_dist("Bernoulli")
# 
# train_params <- training_params(num_trees = 2000, interaction_depth = 3, 
#                                 num_train=nrow(data), num_features = 3)
# fit <- gbmt(Y ~ X1 + X2 + X3, distribution=bern_dist, data=data, weights = w, 
#             offset=offset, train_params = train_params,
#             cv_folds=5, keep_gbm_data=TRUE)
```

---
title: "Trainer"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Packages

```{r}
pacman::p_load(tidyverse, purrr, furrr)
devtools::load_all()
```


# Main Function

```{r}
split_sample <- function(x, y, id = NULL, val = F, oos = F, balance = NULL){
  sp <- splitter$new()
  sp$set(x, y, id)
  sp$split(val, oos, balance)
  return(sp)
}
```



# Test Data

```{r}
df <- tibble(
  id = sample(1:100, 1000, replace = T),
  split_id = sample(1:3, size = 1000, prob = c(.8, .1, .1), replace = T),
  x1 = rnorm(1000),
  x2 = rnorm(1000),
  z = 2*x1 + 3*x2,
  y = ifelse(z > mean(z), 1, 0) #2*x1 + 2*x2 + rnorm(1000)
) %>% 
  select(split_id, y, id, contains("x")) %>%
  glimpse
```



# Trainer

## Keras

```{r}
keras_trainer <- R6::R6Class("keras_trainer",
  private = list(
    class_weight = function(){
    
      self$param$class_weights <- self$data$train$y %>%
        tibble(var = .) %>%
        count(var) %>%
        mutate(n = max(n)/n) %>%
        pull(n) %>%
        as.list %>%
        set_names(c("0", "1"))
    },
    class_weights = function(){
      
      self$param$class_weights <- self$data$train$y %>%
        as_tibble() %>%
        set_names(1:length(.)) %>%
        imap(~{
          out <- .x  %>%
            tibble(var = .) %>%
            count(var) %>%
            mutate(n = max(n)/n) %>%
            filter(var == 1) %>%
            pull(n)
          return(out)
        }) %>%
        c(list(`0` = 1), .)
      
    },
    get_param = function(model, param){
    
      out <- formals(model) %>%
        imap(~{
          if(.y %in% names(param)){
            .x <-  param[[.y]]
          }
          return(.x)
        })
      return(out)
    },
    comp_param = function(param1, param2){
      not_included <- param2[!(names(param2) %in% names(param1))]
      out <- param1 %>%
        imap(~{
          if(.y %in% names(param2)){
            .x <-  param2[[.y]]
          }
          return(.x)
        }) %>%
        c(not_included)
      return(out)
    },
    compile = function(){
      
      model_param <- private$get_param(self$param$model, self$param)
      self$param <- private$comp_param(self$param, model_param)
      self$param$model <- do.call(self$param$model, model_param) %>%
        keras::compile(
          loss = self$param$loss,
          metric = self$param$metric,
          optimizer = self$param$optimizer
        )
      
    },
    set_defaults = function(){
      
      if(is.null(self$param$batch_size)) self$param$batch_size <- 30
      if(is.null(self$param$verbose)) self$param$verbose <- 1
      if(is.null(self$param$callbacks)) self$param$callbacks <- c(keras::callback_early_stopping(monitor = "val_loss", patience = 1, mode = "auto"))
      
    },
    set_weights = function(){
      if(!is.null(self$param$class_weights)){
        if(length(colnames(self$splits$train$y)) == 0){
          private$class_weight()
        } else {
          private$class_weights()
        }
      } else {
        self$param$class_weights <- NULL
      }
    }
  ),
  public = list(
    model = NULL,
    param = NULL,
    data = NULL,
    initialize = function(objective) {
      
      if (objective == "linear") {
          self$param$output_fun <- "linear"
          self$param$loss <- "mse"
          self$param$metrics <- "mean_squared_error"
          self$param$optimizer <- "adam" #optimizer_rmsprop(),
      } 
      # if (private$objective == "mixture"){
      #   self$param$output_fun <- "linear"
      #   #self$param$loss <- 
      #   self$param$metrics <- "mean_squared_error"
      #   self$param$optimizer <- "adam" #optimizer_rmsprop(),
      #   self$predict <- predict_mixture_keras
      #   
      # }
      if (objective == "binary") {
          self$param$output_fun <- "sigmoid"
          self$param$loss <- "binary_crossentropy"
          self$param$metrics <- "accuracy"
          self$param$optimizer <- "adam"
      }
      
      if (objective == "categorical") {
          self$param$output_fun <- "softmax"
          self$param$loss <- "categorical_crossentropy"
          self$param$metrics <- "accuracy"
          self$param$optimizer <- "adam"
      }
    },
    set = function(param, data){
      
      self$data <- data
      self$param <- c(self$param, param)
      self$param$model <- param$model
      
    },
    fit = function(){
      
      ### defaults if missing
      private$set_defaults()
      
      # define classweights by default
      private$set_weights()
      
      ### compile keras model
      private$compile()
      
      # val data?
      if(!is.null(self$data$val$x)){
        val_data <- list(self$data$val$x, self$data$val$y)
        self$param$validation_split <- NULL
      } else {
        self$param$validation_split <- .2
        val_data = NULL
      }
  
      
      keras_param <- list(
        self$param$model, 
        self$data$train$x, 
        self$data$train$y, 
        batch_size = self$param$batch_size,
        #shuffle = T,
        class_weight = self$param$class_weights,
        epochs = self$param$epochs, # old: x$epochs %error%  in combination with early stoping: free lunch!
        callbacks = self$param$callbacks, 
        validation_split = self$param$validation_split,
        validation_data = val_data, 
        verbose = self$param$verbose
      ) 
      
      do.call(keras::fit, compact(keras_param))
    },
    predict = function(x_test = NULL){
      
      if(is.null(x_test)) x_test <- self$data$test$x
      if(is.null(x_test)) return(message("No new data found"))
      
      pred <- predict(self$param$model, x_test)
      
      if(ncol(pred) == 1) pred <- pred[,1, drop=T]
      
      return(pred)
    }
  )
)
```



```{r}
devtools::document()
### Input
custom_keras <- function(input_dim = 20, output_dim = 1, output_fun = "sigmoid"){
  keras::keras_model_sequential() %>%
    keras::layer_dense(units = 10, activation = "relu", input_shape = input_dim) %>%
    keras::layer_dense(units = output_dim, activation = output_fun)
}
pkeras <- list(input_dim = 3, output_dim = 1, output_fun = "sigmoid", model = custom_keras)

### Data
sp <- split_sample(x = df[,-1:-2], y = df$y) %>% glimpse

### Modelling
trainer <- keras_trainer$new("binary")
trainer$set(pkeras, data = sp$splits)
trainer$fit()
pred <- trainer$predict()

### Eval
e <- evaluator$new("binary")
e$eval(target = sp$splits$test$target, pred)
e$preds
e$plots
#trainer$predict(sp$splits$train$x)
```





